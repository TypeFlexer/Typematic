% !Tex root = checkedc.tex

\chapter{Related work}
\label{chapter:related-work}

The C family of programming languages is used widely for system
programming.  The lack of
bounds checking in C and related languages such as C++ has had
serious practical consequences for computer security and software 
reliability.   There has been extensive research and work in industry 
addressing the lack of bounds checking in C. 
In this chapter, we discuss related work and explain how
Checked C relates to it. 

C programs also make extensive use of void pointer types. This allows
static typechecking to be bypassed and create the possibility of
type confusion.  We also discuss related work on detecting or
preventing type confusions due to void pointers and casts.

The work falls into 5 categories:
\begin{itemize}
\item Runtime-based approaches.  The compiler or
runtime system or both are changed to detect out-of-bounds access at runtime.
No source code changes are made.  These approaches rely on general mechanisms for
detecting bounds errors, such as side-data structures or altering
the representation of pointers.  These approaches
are attractive from an ``ease-of-adoption'' perspective, but they have
performance  and compatibility issues that make them unsuitable for use 
for production code.  They are widely used for testing purposes.
Checked C requires program source code changes,
but avoids the overhead of general-purpose mechanisms, making it suitable
for production code.
\item Security mitigations.  These attempt to detect memory
corruption after it has happened or prevent an attacker from taking over a system
after a memory corruption.
Some of them are used widely in practice, such as data-execution prevention,
stack canaries, and address-space layout randomization.  They do not protect
against data modification or data disclosure attacks in general and are 
vulnerable to being circumvented using those attacks.  Checked C addresses
the underlying issue of data modification or data disclosure via
out-of-bounds memory accesses, closing off an entire line of attack.
Checked C programs remain vulnerable to memory management errors and
incorrect pointer type casts, so Checked C complements these mitigations; 
it does not replace them.
\item Static analysis.  These analyze programs without running
them and identify potential out-of-bounds memory accesses.  They are
attractive from an ``ease-of-adoption'' perspective.  However, they
may both miss errors and report errors that do not exist (false positives).
False positives are a problem because programmers end up ignoring or missing
genuine errors.  Checked C provides a way to write code in C that is guaranteed
to be bounds-checked.
\item Program verification: this is a general-purpose approach
that can be used to guarantee that programs only access in-bounds memory. 
Widespead adoption of verification technology for C has its own hurdles,
requiring that programmers integrate the use of a theorem prover into
development. In contrast, Checked C can be integrated directly into compilers.
Most programming languages rely on runtime checking of bounds.  Checked C
matches this level of functionality and does not require changes at uses of
pointers and arrays.  
In contrast, programmers using verification technology who merely want runtime
checking of bounds would have to manually put checks in at uses of pointers and arrays.
\item Programming languages: several projects have proposed type-safe 
dialects of C.  The Cyclone and CCured projects propose changes that
break compatibility, either by changing the meaning of existing programs
or changing data layout.  The Deputy project proposes using dependent types
to declare the bounds of pointers without changing data layout.  Checked C
builds on the Deputy approach, but decouples bounds information from types.
This avoids the complexity of modifying the C type system to support
dependent types.  Dependent types would likely be difficult for programmers to understand.
\end{itemize}

\section{Runtime-based approaches}

The C specification leaves the semantics of out-of-bounds pointer
arithmetic and out-of-bounds pointer access undefined.  This makes it possible 
to implement runtime-based approaches that are consistent with the semantics of
C and that do not require source code changes.   It is also possible
use probabilistic or fault-tolerant approaches to tolerate out-of-bounds
memory accesses.

There are two general approaches used in runtime-based approaches.
The first is to change the representation of pointers to carry
bounds with them.  The second uses a side-data structure to
hold bounds information.   There are several different kinds of side-data
structures that can be used.  One kind tracks blocks of memory that are valid to
use.  The blocks are typically powers of 2 and can range in size
from a byte to up to 32 bytes. This is used to track ``red zones'' around objects that
are invalid to access.  These are useful for detecting buffer overruns caused
by loops, but may not detect other out-of-bounds accesses.  Another kind tracks the
start and end locations of objects and can be used to provide object-level bounds checking.   A third approach uses a shadow memory to track bounds information for individual pointers, splitting the bounds information from the pointer.

These approaches work well for testing purposes, but they have issues that
prevent them from being used for production systems.  There are issues
with performance, backwards-compatibility, and constraints on memory
management and layout.  Most fundamentally, all of these approaches add
runtime data to programs, which in turn increases memory accesses, processing 
time, and memory footprint.  They have to pay for generality regardless of
whether it is needed or not. In contrast, Checked C allows re-use of existing data.
It incurs no data overhead for constant-sized arrays or pointers on which no pointer arithmetic is done.   Approaches that change
pointer representations have difficulty interoperating with existing
systems because that changes the layout of data that must be passed
across boundaries.  Approaches that track data on the side generally
require modifying memory allocators.  A number of these
approaches constrain memory layout and object sizes to speed up the lookup of
the side data.

These approaches also take control of checking away from the programmer.
Runtime-based checking {\em always} checks, unless the check can be proven 
redundant by compiler optimization.   With Checked C, programmers have
a range of options   The default behavior is to check. For performance-critical
code, programmers can rely on static checking that proves the checks
are unnecessary or omit checks entirely.

We first describe approaches that change the representation of pointers.
The bcc source-to-source translator \cite{Kendall1983} and the rtcc
compiler \cite{Steffen1992} were used to find bounds errors and
other errors during debugging.  Each changed the representations of pointers to be 
3 words: the pointer itself and an upper and lower bound.   
Steffen \cite{Steffen1992} reports that the rtcc compiler generated code
that was 3 times larger and ran about 10 times slower than the original code, 
likely reflecting the simple nature of  the optimizer for the PCC compiler.  Data
layout compatibility is an issue.  Bounds information has to be removed at calls to
standard library  functions and added at returns from standard library functions.
Austin {\it et al.} \cite{Austin1994} describe a pointer representation that
adds a capability in addition to bounds information.  The capability prevents
accesses to de-allocated objects.  The runtime system tracks capabilities that
are valid (memory that has not been deallocated).  

Fail-Safe C is a memory-safe compiler for ANSI C \cite{Oiwa2009}.  It
supports all operations in ANSI C, including cast operations.
It represents pointers as pairs, where each pair consists of the base address
of an object and an integer offset from the base address. 
It changes the representation of integers to be pairs as well so that pointers can 
be cast to integers and back.  It also changes the representation of memory blocks in
C to dynamically track their types.  This
supports the C notion that memory locations are dynamically typed; the type
of the value in a memory location depends on the type of the last value stored there.
It use conservative garbage collection to ensure the safety of memory
deallocations.   With the data layout changes, programmers no longer 
directly control memory representations.  This makes Fail-Safe C unsuitable for low-level  system programming.
It is suitable for applications programs, provided that wrappers for system
calls are provided.  Bytemark benchmarks are 2 to 4 times slower.  OpenSSL RSA
speed tests are 2 to 4 times slower, while AES speed tests are 5 times slower.

Grimmer {\it et al.} \cite{Grimmer2015} propose executing C programs within a Java
Virtual Machine.   They represent pointers as pairs consisting of the base address 
of an object and the element count offset from the base address.  They do not allow
pointers produced by casting integers to pointers to be used to access
memory.

We next describe approaches that use side-data structures.    We first
describe approaches that track what memory is valid to access.
Purify \cite{Hastings1992, Unicom2016} detects some bounds checking problems
as well as uses of uninitialized memory and memory leaks.  It is meant for
use during development and debugging.  Purify uses a table that keeps track
of the state of each byte in memory, using 2-bits to represent the state of
memory.  It inserts a small ``red zone'' before and after each dynamically
allocated object and between statically-allocated objects.   It also
inserts red zones between stack frames.  All memory accesses are instrumented 
to  check or update the state for a byte. 
The instrumentation is inserted by rewriting binary code before linking.
Purify detects out-of-bounds memory reads and writes involving red-zone
memory.   It does not detect out-of-bounds
reads or writes that occur entirely within valid memory for other objects
or stack frames. It cannot detect out-of-bounds memory accesses at
the sub-object level.
Hastings {\it et al.} report a slowdown of more than 2 times due to
the instrumentation.  However, this includes additional checking for memory leaks 
and the use of uninitialized memory.

There are a number of other similar commercial or open-source tools available
that detect out-of-bounds memory accesses.
Tools based on binary rewriting include Bounds Checker \cite{BoundsChecker2016}, 
Dr. Memory \cite{Bruening2011,DrMemory2016}, Intel Inspector \cite{Intel2016},
Oracle Solaris Studio Code Analyzer \cite{CodeAnalyzer2016},
and Valgrind Memcheck  \cite{Nethercote2007, Valgrind2016}.  Insure++ 
\cite{Insure2016}
inserts instrumentation using source-to-source rewriting.  It also provides
a mode that does not require recompilation, although details of how that
works are not described.

AddressSanitizer \cite{Serebryany2012} is a tool similar to Purify that is
incorporated into the LLVM and GCC compilers.  It uses a table
stored in shadow memory that tracks that state of 8-byte chunks of memory, using
1/8 of the virtual address space.  It also relies on inserting ``red zones''.
Because it has been implemented in a compiler, it is able to place red zones
around stack-allocated objects.   It cannot detect out-of-bounds memory
accesses at the sub-object level and cannot reliably track out-of-bounds accesses
for objects smaller than 8 bytes.  For SPEC CPU2006, average program execution time
increases by 73\% when checking reads and writes and 26\%
when only checking writes. For SPEC CPU2006, average memory usage is 
3.37 times larger.

Light-weight Bounds Checking \cite{Hasabnis2012} is an optimized implementation of bound
checking that uses ``red zones''.
It focuses solely on bounds checking.  It uses a bitmap to track
which bytes in memory correspond to allocated objects and which do not.
It uses a two-level table to avoid allocating a table equal to 1/8 of the
address space.  It optimizes memory reads by filling red zones with special
values.  If a memory read returns a special value, then a check is done to
ensure that the address was not a red zone address.  It is implemented in a compiler and can guard stack objects.  For SPECINT 2000, average 
program execution time increases by 23\%. For SPECINT 2000, its memory 
overhead ranges between 0.2\% and 44\% with an average of 8.5\%.

Next, we describe approaches that use side-data structures to track
object bounds.   
Jones and Kelly \cite{Jones1997} use a splay tree to track the bounds for
objects in a side data structure.  They insert 
checks for pointer arithmetic to make sure that pointers stay within valid bounds for
objects.  If a pointer goes out of bounds, it is converted to a value that cannot
be dereferenced and that is not allowed to go back in bounds. 
They change checked code to call modified versions of \keyword{malloc},
\keyword{free}, and system-level allocators.  The compiler modifies
code generation for stack allocated objects to call functions that update the 
bounds information. In a production system, all custom allocators would have to be
modified to update the object bounds information.  

Jones and Kelly implememented their approach in GCC.   The approach easily allows
interoperation between checked and unchecked code.  Objects that are not allocated by
checked code are not tracked.  Untracked and tracked objects are treated as distinct memory regions.  Operations in checked code on untracked objects are not allowed to produce pointers to checked objects, providing some protection to untracked objects.  

Their implementation increases program running
times by a factor of 7.7 to 12 \cite{Nagarakatte2009} for  a set of
23 benchmarks that includes programs from SPEC and the Olden benchmark.
There are other drawbacks to this approach.
Because bounds are tracked at the granularity of objects, it
cannot track bounds for arrays nested within structures. There are also
issues with handling pointers to one element past the last element of
an array.  Their approach inserts padding between objects to have a gap.  In some
cases, such as parameters passed on the stack, this is not possible.

The C Range Error Detector (CRED) \cite{Ruwase2004} extends the Jones and Kelly
approach to tolerate out-of-bounds pointers.  They observe that 12 out of
20 open source programs totalling 1.2 million lines of C code break when
out-of-bounds pointers are not allowed.   This observation supports the
design decision in Checked C to allow out-of-bounds pointers and only
check bounds at memory dereferences.

CRED uses a proxy object that
tracks the original pointer value and the object with which it is associated
so that the pointer can go back in bounds.  When a pointer goes out of
bounds, it is replaced with the proxy object address.  This requires
additional checks on comparisons and pointer arithmetic computations.
They find similar increases in program running times to those of Jones and
Kelly.  They suggest
limiting bounds checking to only character pointers. With this restriction,
the increase in execution time  ranges from 1\% to 130\% on a set of real-world
programs.  They do not report on changes in memory usage.

Dhurjati and Adve \cite{Dhurjati2006} describe an optimized implementation of
CRED.  It relies on a whole-program analysis to partition
objects into separate pools.  The pool information is used to partition
the splay tree and  to avoid having to create entries for single-element arrays 
or scalar objects in the splay tree.

Baggy Bounds Checking \cite{Akritidis:2009:BBC:1855768.1855772}
provides a faster implementation
of the side data structure in Jones and Kelly.  The implementation 
calculates the bounds for any pointer in constant-time.   To achieve this,
the implementation constrains object sizes to be powers of 2.  It also reserves 
1/\var{n} of the virtual address space for a table, where \var{n} is the smallest 
allowed object size (16 in the implementation).  The table stores the size of the
enclosing object (if any) for each \var{n}-byte chunk of memory.   Baggy Bounds 
Checking increases average execution time for SPECINT 2000 benchmarks by 60\%. 
It increases average memory usage by 20\%.

Paricheck \cite{Younan2010} stores a 2-byte label for each 32-byte chunk of memory.
It reserves 1/16 of the virtual address space for a table and increases
the memory object allocation size to 32 bytes.
It checks that pointer arithmetic stays in bounds by checking that the
original pointer and the pointer computed by pointer arithmetic have the
same label. For SPEC CPU2000, it increases average execution time 49\% and
average  memory usage by 9.5\%.

Low Fat Pointers \cite{Duck2016} encode bounds information into 64-bit pointers by
dividing memory into \var{m} regions of 4 GBytes each and storing
the region information in the upper 32-bits of the pointer.  Each
region contains objects of some size $k$ that are aligned to $k$.
A table maps regions to their sizes.  They measure the performance of 
SPEC 2006 programs and find that checking all pointer reads and writes
increases average program execution time by 56\% and checking only writes 
increases average execution time by 13\%. In \cite{Duck2017}, the authors extended 
Low Fat Pointers to also provide stack bounds protection, incurring a 17\% overhead when checking 
only writes. Low Fat Pointers for heap \cite{Duck2016} and stack \cite{Duck2017} bounds protection were
integrated and released as an open source research prototype available at 
\url{https://github.com/GJDuck/LowFat}.

Finally, we describe shadow memory approaches.
Patil and Fischer \cite{Patil1997}
implement bounds checking using a second process that follows the execution 
of the original process.  They separate the bounds and lifetime information
from the original pointer and use shadow heaps and shadow stacks in the
second process to track it.  
SoftBound \cite{Nagarakatte2009} tracks bounds information
for each  memory location that contains a pointer by using a side table.  
As an example, given a pointer variable,
the system tracks the bounds based on the address of the pointer variable.
This allows the system to track sub-bounds within objects.  The calling
convention for values passed in registers is modified to have additional
parameters for bounds.  SoftBound uses either a hash table or
a shadow copy of memory to track the bounds information.   The shadow
copy of memory generally has better performance.  With the shadow
copy of memory, SoftBound increases average program execution time for
a set of benchmarks by 67\%.   SoftBound can check only writes, in which
case average program execution time increases by 22\%.   SoftBound increases 
average memory footprint by 64\%, although it can increase it by up to 200\%.

MemSafe \cite{Simpson2013} provides bounds safety and lifetime safety for C.
It too tracks bounds information for each memory location that contains a 
pointer using a side table.  It uses capabilities to prevent the use of
invalid pointers to access or free memory.  It stores capabilities in the side 
table and keeps a map from valid capabilities to the bounds of the associated objects.
MemSafe uses whole-program optimization to optimize dynamic checks, to avoid placing bounds and capability information in the side table, and to avoid tracking 
capabilities. For 30 programs from the Olden, 
PtrDist, and SPEC benchmarks, MemSafe increases average execution time by 88\%
and average memory usage by 49\%.

An alternate approach to dealing with out-of-bounds memory accesses is to
use probabilistic techniques that allow programs to tolerate out-of-bounds
memory writes.   DieHard \cite{Berger2006} randomizes the location and spacing of 
objects on the heap.  It allocates a heap at least as twice as large as needed for
program data.    This causes some out-of-bounds writes to modify memory in gaps
between objects.  This provides good protection against modest buffer overflows. 
To protect against out-of-bounds memory reads, programs
can be replicated, run in parallel, and text outputs compared.  The replicated
approach does not work for programs that have non-deterministic output. It is 
unclear how it would work for programs that are interactive.

\section{Security mitigations}
Security mitigations are another approach to dealing with programs that corrupt memory
through out-of-bounds writes or that expose data through out-of-bounds
reads.   These employ runtime-only mechanisms to detect that memory has been
corrupted or to prevent an attacker from taking control of a system after
memory has been corrupted.

Attackers can use incorrect programs to attack the security of 
computer systems in the following ways:
\begin{enumerate}
\item  Execution of arbitrary code: An attacker may be able to inject arbitrary machine code into a process and have the process  execute that code.
\item Control-flow attacks: this a more subtle attack that avoids the need to inject arbitrary machine.  An attacker manipulates program state to
stitch together execution of a program of the attacker's choosing
using existing machine code.
For example, in return-oriented programming, an attacker finds segments of useful
machine code that end in return instructions. The attacker manipulates the state of 
the program call  stack to execute a series of small pieces of machine code and execute
a arbitrary program.  There are other ways to manipulate program state to
change control-flow, such as changing the target of an indirect function call.  
This can be done by 
overwriting a function pointer or the virtual table of an object.
\item Data modification: an attacker may be able to write data to a process, causing the process to take an incorrect action on behalf of an attacker.
\item  Data disclosure: an attacker may be able to read data from a process 
and obtain data, including data that is meant to be confidential.  
\end{enumerate}

Security mitigations that have been developed and deployed in
practice include data execution prevention (DEP), 
address-space layout randomization (ASLR), stack canaries,
shadow stacks, and control-flow integrity (CFI).   DEP, ASLR, and CFI focus
on preventing execution of arbitrary code and control-flow modification.  
Stack protection mechanisms focus on protecting data or return addresses
on the stack. 

Checked C provides protection against data modification and
data disclosure attacks, which the other approaches do not.
Chen {\it et al.} \cite{Chen2005} show that data modification
attacks that do not alter control-flow pose a serious long-term threat.
The Heartbleed attack on OpenSSL~\cite{Heartbleed} illustrates the damage that
is possible from even data disclosure attacks.

Checked C addresses the fundamental problem, which is incorrect programs
with undetected errors. Checked C enhances existing security mitigations by
providing protection against data modification and data disclosure attacks.
ASLR, DEP, CFI, and stack canaries can be 
defeated by determined attackers using data modification and data disclosure
attacks.  Shadow stacks do not protect stack-allocated buffers or arrays, 
heap data, and statically-allocated data.

If a C program and its libraries use only checked pointers and checked arrays and 
the program is free of pointer type 
cast or memory management errors, Checked C provides a strong
guarantee about memory reads and writes. Any pointer must have been constructed
via a  series of operations from a pointer to an object.   Checked C ensures 
that the constructed pointer only accesses memory within that object.

Checked C programs 
remain vulnerable  to incorrect pointer type casts, memory management errors, 
and race conditions that invalidate bounds information. They are also
vulnerable to data modification by unchecked code in the same address space.

In the remainder of this section, we review security mitigations that
have been deployed in practice. We discuss what they protect against
and how they can be defeated by data modification and data disclosure attacks.

Data execution prevention (DEP) relies on hardware and OS-based approaches
to prevent attacks that inject machine code into a process and then
execute it.   At the hardware level, virtual memory support provides
a ``no execute'' bit for each virtual memory page that forbids the execution
of instructions located on that page of virtual memory.  This bit can be set by
default for program stacks and for memory when it is first allocated. A process
may have to request specifically that an area of virtual memory be made
executable.  
DEP does not defend against control-flow,  data modification, and data disclosure
attacks.  Control-flow attacks can be used to execute a system call and disable 
data execution protection.    

DEP is deployed widely in production.  It has been very
successful protecting against ``classic'' buffer overrun attacks that overwrite 
stack contents with machine code and also overwrite the return address to jump to 
the machine code.
Hardware support was incorporated into 32 and 64-bit processors
for the Intel x86 architecture in 2005 and 2003, respectively.  Windows
and Linux have supported it from that time as well.  For 32-bit programs on
Windows, programs must opt-in.  That is the default for programs compiled
with the Microsoft Visual C++ compiler.

Software can emulate ``no execute'' support or provide approximations of it
when hardware support is not available. Software fault isolation
\cite{Castro2009, Erlingsson2006,McCamant2006,Wahbe1993,Yee2009} injects checks
into machine code, either during compilation or by rewriting binaries.
It implements address spaces for fault isolation in software. Software components communicate via remote-procedure calls. Implementing address
spaces in software avoids expensive hardware context switches.
Wahbe {\it et al.} \cite{Wahbe1993} describe how execution of data can be
prevented by placing code and data in separate areas of memory and checking the
targets of  indirect jumps.  Code running in a software-fault isolated address
space, like code running in a hardware-supported address space, is vulnerable to 
control-flow, data modification,  and data disclosure attacks.

ASLR \cite{PaX2003,WikipediaASLR} provides protection against 
control-flow attacks.   All major operating systems provide
some form of ASLR. In ASLR, code
sections from executable files are loaded in random locations in the
address space of a process.  In addition, data sections, stacks, and
dynamically-allocated data are also placed in random locations in the address
space of a process. This makes it harder for an attacker to identify the locations of fragments of code to be used in return-oriented programming.  However, ASLR does
not protect against data modification or data disclosure attacks. For example, data 
may be located on the stack adjacent to a
variable that is subject to a buffer overrun; the buffer overrrun can be
be used reliably to overwrite or read the data.   

ASLR can be compromised by data disclosure attacks.  An attacker can obtain the
location of a code section by reading data from the stack, for example, and obtaining function return addresses.  The attacker can then craft an attack based on that data. 
A number of proposals suggested finer-grained  randomization of code layout
\cite{Bhatkar2005, Hiser2012, Kil2006, Pappas2012,Wartell2012}.
For example, code layout can be randomized at the instruction level or
basic-block level. These too are vulnerable to 
data disclosure attacks \cite{Snow2013}.

The idea behind ASLR is to use randomization to protect
code and data addresses.  For randomization of addresses to be effective, 
it requires hardware  architectures that have 64-bit addresses and 
virtual memory support \cite{Shacham2004}.  With 32-bit addresses, 
there is typically  at most 16 bits of entropy available for
virtual memory allocation.  Data is allocated at virtual memory
page granularities or multiples of virtual memory page granularities
and using upper bits fragments the virtual address space, which
could cause large virtual memory allocations to fail.  This
amount of entropy is not enough to defend systems deployed at scale on
the Internet.  An attacker only needs on average 32,768 probes for one system or 
conversely 32,768 target systems to compromise one of them through a
brute force attack.
This limits the usefulness of ASLR in embedded domains where 
64-bit address spaces are uncommon.  

Another set of ideas is to protect the data that contains 
code addresses.  Some ideas aim to protect against modification;
others aim to protect against disclosure of the code addresses.
Stack canaries \cite{Cowan1998, Dang2015, Petsios2015} provide 
some protection against injection of 
arbitrary code and control-flow attacks.
They protect against attacks that modify return addresses
on call stacks by overrunning the bounds of a stack-allocated array.  
This kind of overrun can happen when using C string functions that do
not validate parameter lengths, for example.   A
compiler injects code at function entry points and return
points. Entry point code places a token on the call stack
next to the return address.  Return point code checks that
the token has not been modified before executing the return
instruction.  The value of the token may be computed at run time
and selected to have specific properties that aid in the detection
of overflow attacks.  This form of stack protection is available as a
compiler option for the GCC and Microsoft Visual C++ compilers.

Canaries do not provide protection against data modification attacks
that modify only the contents of stack-allocated variables, that
precisely modify return addresses, or that modify other areas 
of memory such as the heap and global variables. They also do not
provide protection against data disclosure attacks. 

An alternative to stack canaries is shadow stacks \cite{Abadi2005, 
Baratloo2000, Bhatkar2005, Chiueh2001, Corliss2005, Erlingsson2006,
Frantzen2001, Kuznetsov2014}.
With shadow stacks,
the stack is split into two stacks.  One stack is the secure stack. It
is accessible via a dedicated hardware register and placed at a random
location in memory.   The location of the secure stack is protected
against disclosure by guaranteeing that all memory accesses
to the secure stack are in bounds.  For example, only scalar variables whose
addresses are not taken may be stored on the secure stack. In addition, 
the addresses of locations on the secure stack are only stored on the secure
stack.   The other stack is the insecure stack.  Its location is not
protected against disclosure.

In some approaches, the secure stack is used to only hold return addresses.
A return address is stored on the regular stack as well and there is a
check that the return address is unmodified before doing a return.
This incurs an overhead of about 10\% because of the cost of having two
stacks and checking return addresses \cite{Dang2015}.   The
regular stack can be used as the secure stack and variables
that may have out-of-bounds memory accesses or whose addresses are taken
can be stored on the insecure stack \cite{Bhatkar2005, Kuznetsov2014}.  
Kuznetsov {\it et al.}   
\cite{Kuznetsov2014} observe that this improves performance because
many small stack frames do not even need shadow stack frames.  For
SPEC CPU 2006 benchmarks, they found that the performance cost 
ranges from -4.2\% to 4.1\%,
with an average cost of less than 0.1\%.  They speculate
that performance improvements are due to improved data locality for
stack accesses from the placement of large arrays on the insecure stack.
This form of stack protection is available as a compiler option 
for the LLVM compiler.

Shadow stacks do not prevent data modification and data disclosure attacks
against the insecure stack or other areas of memory such as the heap 
and global variables.  Shadow stacks also have backwards compatibility
problems; all code used by a thread must be converted to prevent disclosure
of the shadow stack location for the thread.  

A shadow stack that uses the regular stack can be attacked in
a subtle way, even if all memory accesses to the stack
are guaranteed to be in bounds.  An attacker can cause a calling convention
mismatch, where the caller of a function and the called function disagree on
the size of argument data that is passed on the stack or who is expected to
make adjustments to the stack pointer.  This corrupts the stack pointer, 
allowing a data modification or data disclosure attack against the shadow stack, including overwriting return addresses \cite{Goktas2014}.

Like ASLR, shadow stacks that use the regular call stack may be vulnerable to 
brute-force data disclosure attacks on systems with 32-bit addresses.
For example, on a 32-bit Windows system, the smallest possible
stack size is 64K and the uppermost 1 GByte of virtual address space
is not available by default.  If an attacker is able to read a byte 
in memory at an attacker-selected location and the attacker randomly picks
the location,  an attacker has a 1 in 49,152 
chance of reading a byte that is on a virtual memory page that contains a 
shadow stack location.  With reads of nearby locations, an attacker can likely 
determine if the page containing the byte contains part of a stack.

Control-flow integrity (CFI) also provides some protection
against return-oriented programming attacks \cite{Abadi2005}.  
CFI adds runtime checks to machine code to ensure
that a program follows an approximation of the valid control-flow of the program. 
There have been many follow-up papers \cite{Akritidis2008, Li2011, Li2010,
Mashtizadeh2015, NiuPLDI2014, NiuCCS2014, Niu2015, Sadeghi2015, Tice2014, 
vanderVeen2015, Wang2015, Wang2010, Zeng2011, ZhangSP2013,Zhang2015, ZhangSEC2013}.
In the description in \cite{Abadi2005}, a compiler computes the target addresses of each 
indirect  function call and each return instruction.  The
compiler groups addresses into equivalence classes: addresses are in the same 
class if they may be the target of the same call or return instruction.  This is
used to generate unique identifiers for the runtime control-flow check. 
Many different variants of CFI have been proposed, including coarse-grained CFI
implementations that are less precise than the original description \cite{Wang2015,
ZhangSP2013, ZhangSEC2013} 
as well as fine-grained CFI implementations \cite{Tice2014,Wang2010} 
and even context-sensitive CFI implementations \cite{vanderVeen2015}.
CFI has been applied in production C and C++ compilers to indirect function
calls and not applied to return instructions
\cite{GCCCFG2016,LLVMCFG2016,MicrosoftCFG2016, Tice2014}.   Return instructions
are protected via other means such as stack canaries or shadow stacks.  
CFI is implemented in various forms in production versions of the
GCC, LLVM, and Microsoft Visual C++ compilers.

CFI does not defend against data modification or data disclosure attacks. 
It is also vulnerable to data modification attacks.  Determined attackers can
use a data modification attack to still construct a control-flow attack
\cite{Carlini2015, Conti2015, Evans2015, Goktas2014}.
The runtime control-flow checks are imprecise because the control-flow graph (CFG)
is an approximation of the actual control-flow that is possible for
a program.  The computed CFG must allow at least all legal executions of a
program. In fact it allows invalid executions of a program too.  An
attacker can take advantage of that difference to control execution of the program
via a data modification attack.

CFI is based on the assumption that a precise control-flow graph can be
constructed for C and C++ programs.  According to Evans {\it et al.} \cite{Evans2015},
``this assumption is tenuous at best''.  It is difficult to construct
a precise CFG for programs with pointers that use function pointers and
object-oriented language features.  Coarse-grained CFI implementations make the
checked CFG even less precise.  They do this to reduce  the cost of runtime checking or 
because of difficulties computing a precise CFG.  For example, binary rewriting
approaches have difficulty precisely computing possible targets for indirect jumps.
This allows even more invalid executions.  Coarse-grained CFI implementations
were  first shown to be vulnerable to an 
attack based on the imprecision of the checks \cite{Carlini2014,Davi2014,Goktas2014}. 
Fine-grained CFI implementations are also vulnerable to the same kind of attack
\cite{Carlini2015,Evans2015}.

Evans {\it et al} \cite{Evans2015} explain how constructing the CFG relies on a
points-to analysis for pointers.  Sound and complete points-to analysis is 
undecidable \cite{Ramalingam1994}, so points-to analyses implemented in compilers 
must approximate the actual points-to behavior of programs.  This leads to 
imprecise CFGs in practice.

It is also difficult to construct a precise CFG in the presence of 
separately-compiled dynamically-loaded libraries \cite{NiuPLDI2014} or just-in-time
compiled code \cite{NiuCCS2014}.  Tice {\it et al.} \cite{Tice2014} discuss the difficulties of implementing CFI for programs that use dynamically-loaded libraries. 
Hand-written assembly code also causes problems for constructing a precise CFG.

\section{Static analysis tools}

Static analysis tools are used widely to identify defects in C and C++ programs.
The tools take the source code for a program (or, less frequently, binary
code) and attempt to find possible bugs.  They do so by analyzing the source
code for the program.  There are many available commercial static analysis tools
available for C and C++, incuding CodeSonar, Coverity Static Analysis, HP Fortify, 
IBM Security AppScan, Klocwork, Microsoft Visual Studio Code Analysis for C/C++, and Polyspace Static Analysis \cite{Bessey2010,Bush2000,Emanuelsson2008}.    
These tools find many different kinds of bugs, including out-of-bounds array accesses.

Static analysis tools use several different approaches to identify bugs,
including dataflow analysis \cite{Aho2007,Shahriar2010}, simulating program 
execution \cite{Bush2000},  abstract interpretation \cite{Cousot1977}, 
and model checking
\cite{Larus2004}. At a high level, they build a  model of the program and 
prove properties about the model.  This is a large area of study and describing 
it is well beyond the scope of this related work section. We focus only on how
Checked C relates to static analysis tools.

Static analysis tools are imprecise.   They may report that a program
may have a defect, when it does not have that defect.  The imprecision
is inherent in static analysis.  Deciding whether a program has a specific defect is
undecidable in general.  When a static analysis tool reports that a program has a 
defect  and it does not, that is called false positive.

False positives are a significant problem for static analysis tools. Programmers need
to spend time investigating them and consider it a waste of time when they discover
that a ``bug'' is a not actually a bug.   Future runs of the tool still report
the potential defect, so programmers need to suppress or ignore the defect.
More problematically, programmers cannot distinguish easily between false positives
and true positives, so they  ignore or suppress genuine bugs \cite{Bessey2010}.
Tools themselves may suppress error messages for true positives to avoid
too many false positives.

Most static analysis tools make unsound assumptions about C programs.
For example, they may inspect only a limited number of paths through
a function \cite{Bush2000}.  They may also assume that signed integer arithmetic
expressions in C do not overflow.  This is because it would be very difficult to
prove in general that C arithmetic stays within bounds.

There are a few sound static analysis tools that aim to detect all
possible out-of-bounds memory accesses in C programs with few false positives.
Astr\'{e}e \cite{Astree2016,Blanchet2003,Delmas2007}
and Polyspace Code Prover \cite{Polyspace2016} use 
abstract interpretation and other static analyses to diagnose possible integer
overflows, division-by-zero, and out-of-bounds
poointers in embedded C and C++ software.  They can be used to show that a program is 
free from runtime errors. They each produce output that classifies code as free of
runtime errors, definitely faulty, definitely unreachable, or possibly containing a runtime error.  They are ``black boxes'': a programmer must trust the output of the
tool.  In the case of Astr\'{e}e,  extensive detail about its analyses are available,
but one must still trust its implementation.  In the case of Polyspace, details
about the analyses are not available.

To summarize, static analysis tools may miss errors and report errors that do not exist
or errors that are ignored by programmers.  They also function as ``black
boxes'' not subject to independent verification.

Checked C occupies a different design point than static analysis tools.
It provides a way to write C code that is guaranteed to to be bounds-checked.
It does not miss bounds errors for checked pointer types and it does static 
checking of bounds information in a sound fashion for sequential
C programs.  Checked C avoids problems with false positives
by deferring bounds checks to runtime.  It minimizes the information that
must be checked statically by limiting it to bounds information.
For cases where bounds checking failures are unacceptable, it allows
programmers to opt-in to completely static checking.  Finally, the checking is
done using rules that are part of the language definition.

\section{Program verification}

Static analysis tools take programs ``as is'' and try to prove
facts about them.  Sound static analysis tools that produce no or few false
positives act as program verifiers, of course.  Program verification 
adds a specification of program behavior and programmer involvement
in constructing a proof that the program satisfies the specification.
Program verification efforts range in practice from simple efforts to
show that a program is free from runtime faults to costly  
efforts to show full functional correctness.

Checked C aims at a level that does not even exist for most higher-level
languages, where bounds checking is a built-in part of the language
implementation.  The goal is to show that information for checking runtime
faults (that is, bounds information) is correct. Checked C also aims to
support showing parts of programs are free of bounds failures or null
check failures for performance or correctness reasons.

Extended static checking \cite{Flanagan2002} aims to show that programs are 
free of runtime errors such as null check failures and array bounds errors
and free of concurrency errors.  ESC/Java \cite{Flanagan2002}
uses a modular checking approach
where parts of the program (methods) are checked individually.  This enables 
the checking to scale to large code bases or separately-compiled code.
In ESC/Java, a programmer adds annotations in first-order logic to  Java programs to specify invariants 
that assist in showing the correctness of programs.  ESC/Java generates verification
conditions for functions that imply that statements with runtime checks in the
functions will not fail. It then invokes a theorem prover to attempt to 
show that the verification condition is true.  The theorem prover may construct
a proof, construct a counterexample, or not terminate within a desired time bound.
ESC/Java is unsound; it ignores integer overflow, for example.

Checked C, like ESC/Java, uses a modular checking approach.  It checks 
individual functions.  Checked C is intended to be sound for single-threaded 
programs.   Unlike ESC/Java, Checked C uses a propositional logic, instead
of a first-order logic.  Checked C does not have quantifiers such as ``forall''
and ``there exists'' in the language of program invariants.
This means that some statements about data structures
cannot be expressed in Checked C.  Because Checked C uses a propositional
logic, it is decidable whether program invariants for Checked C functions are true.
The variables used in the propositional logic range over finite-sized integers.
Even though checking is decidable, it is NP-complete.  Checked C uses a heuristic-based
approach to check program invariants in reasonable time. Some program invariants that
are true will not be shown to be true.

Checked C proposes extending C with types and static checking to ensure
that pointer and array accesses are always in-bounds or checked to be in-bounds. 
An alternate approach would be to use a general-purpose program verifier.
A C program would be annotated using program invariants about bounds and a 
theorem prover could be used to verify that pointer and array accesses are
always in bounds or checked to be in bounds.  Two program verifiers
for C are Frama-C \cite{FramaC2016} and EscherC \cite{Escher2016}.   

There are two issues with using this approach.  First, it changes how software
development is done by adding a theorem prover to the development process.
This loses the immediacy of having the checking integrated into the compiler.
Theorem provers are slow and prone to long running times and time outs.
Second, most languages with bounds checking rely on runtime checking.  It is difficult
to prove that code always stays in bounds.  The runtime checking would have to be
inserted by the programmer at every use of a pointer produced via pointer 
arithmetic, making programs more verbose and requiring sweeping changes.

\section{Programming languages}

A number of programming languages include pointer types and the ability
to do pointer arithmetic.  This includes languages designed for system
programming such as D \cite{D2016} and Rust \cite{Rust2016}, as well as languages 
that can be  used for system programming if garbage collection is acceptable, 
such as Go \cite{Go2016} and C\# \cite{Csharp2016}.   
These languages strongly discourage the use of ``raw'' pointer
types because of the potential for error.  For example, in C\# and Rust, 
raw pointer types can be used only in \keyword{unsafe} blocks.  Checked C shows
how these languages might be extended to support checked pointer operations.

The Cyclone, CCured, and Deputy projects have proposed new type-safe dialects 
of C.   Cyclone \cite{Jim2002} is a dialect of C that restricts C and also extends it.  The goal is to create a type-safe system programming
language.  Cyclone has language changes that break existing C programs and
cause them to no longer compile.  For example, Cyclone does not allow pointer
arithmetic on unchecked C types.   In contrast, Checked C supersets C. This
makes it possible to use Checked C extensions incrementally, while Cyclone
requires that a program to be converted in its entirety.  Cyclone changes the representation of pointer types that allow arithmetic.  It introduces a new 
pointer type that carries bounds information with it, similar to the \spanptr\
type described in Section~\ref{section:span-design} that we considered adding to Checked C.
This causes  compatibility problems when interoperating with existing
code.   Checked C allows programmers to declare bounds information separately from pointers using \arrayptr\ types.
Cyclone also changes memory management in C.  It extends C with regions
to allow arena-based memory management.  Checked C is addressing the
safety problems in C one at a time, starting with bounds checking. 

Cyclone also proposed using polymorphic types to safely replace 
uses of \uncheckedptrvoid{} and to allow re-use of code \cite{Grossman2006}.  Cyclone
included polymorphic types that provided generic structs and functions, as well as hidden types.  
It did not provide an equivalent of opaque types.  
Grossman \cite{Grossman2006} thoroughly explores the theory of adding polymorphic types
to a C-like language and the design choices made in Cyclone.

CCured \cite{Necula2005} uses whole-program static analysis to identify 
different uses of pointers in C programs. It identifies pointers that are used to
read or write values only (safe pointers), pointers that are used in pointer
arithmetic also (sequence pointers), and pointers that are involved in
possibly non-type safe casts (wild pointers). It uses a multi-machine
word representation for sequence pointers and wild pointers. It also
changes the representation of data pointed to by wild pointers. This
changes data layouts and causes interoperation problems.

Deputy \cite{Condit2007,Feng2006} extends C with dependent types to avoid runtime 
layout changes for pointers involved in pointer arithmetic.  Checked C is directly
inspired by Deputy, although Checked C uses program invariants instead of
dependent types to track bounds information.   The dependent types in Deputy
allow the bounds of
the pointers to be specified as part of the types of the pointers and
the bounds to depend on runtime values. Deputy requires programmers to
annotate function parameters, data structures, and global parameters
with dependent types. It then infers dependent type annotations for
local variables and adds runtime checks to make the dependently-typed
program type check. The runtime checks enforce that pointer values stay
in bounds. The checks apply to pointer arithmetic and pointer
dereferencing.

A dependent type is a type that may depend on a value at runtime.
Dependent types are built using type constructors that are applied to
types and values. The type constructors capture specific properties of
runtime values. For example, Deputy introduces a type constructor
\code|Array| that can be applied to an integer value (the length of the
array) and an element type. \code|Array 5 int| describes the type of
integer arrays with 5 elements. The \code|Array| type constructor can be
applied to a program expression, so the type can depend
on a runtime value. For example, \code|Array n int| describes the type
of integer arrays with \var{n} elements.

There are several problems with using dependent types in C.
First, dependent types are a big change to the
C type system and C type checking. Dependent types are an abstract
concept that may be hard for many programmers to understand. 
Second, even if
programmers can understand dependent types, it makes type checking a
complicated exercise.  To type check a dependently-typed statement, the
type checker must prove that certain runtime invariants are true before
the statement. To illustrate this, consider the type checking rule for
variable assignment from \cite{Condit2007}. This
rule requires that the type checker prove that certain invariants must
be true at runtime before the statement. Type checking becomes entangled
in general reasoning about program invariants.
Finally, using dependent types makes programs verbose: explicit
checks to enforce bounds safety have to be inserted throughout the code. There
are no widely-used languages with array-bounds checking
that requires this level of verbosity. In Java and C\#, the checks are
done implicitly. This is the case in older languages such as FORTRAN
and Pascal, as well.  For these reasons, Checked C uses program invariants
instead of dependent types.

The C11 Standard supports variable-length arrays optionally.  This is a
form of dependent type.  However, type checking for variable-length arrays
in C is weak.  It does not enforce that runtime 
values for lengths in types are equal. Program behavior is undefined if two variable 
length array types are supposed to be compatible and their length expressions differ at
runtime \cite[Section 6.7.6.2]{ISO2011}. In addition, C ignores the variable length 
for function
parameters.  The current support for variable-length arrays does not set a precedent 
for using dependent types to represent bounds information in C.

The CForAll project \cite{Ditchfield1994} proposed extending
C with polymorphic types to provide generic functions.  It also proposed 
adding opaque types, but did not require that the representations be known at declaration time.
CForAll did not provide a way to retrofit existing C code that uses 
\uncheckedptrvoid{} in a binary-compatible fashion.  Opaque types break binary
compatibility by requiring that uses of variables of opaque type obtain size and alignment information
at runtime \cite[page 97]{Ditchfield1994}.  The implementation of generic functions required complex techniques
that break binary compatibility by cloning code or introducing box/unboxing coercions \cite{Bilson2003}.

Havoc \cite{Condit2009} goes beyond Deputy and allows types to be combined with program
verification. It allows a programmer to specify program invariants that
imply type safety and can verify these invariants statically. It can
handle unsafe code such as using a pointer to a field to access a prior
field in a data structure.    Havoc shows that it can be very difficult
to show the type safety statically of  low-level system code. 
The code may be type-safe at runtime and a programmer may intuitively know that it is type-safe.  However, writing down the invariants may be hard and may require deep
knowledge of program verification techniques. For this reason,
Checked C adopts an opt-in model for bounds checking, where code
can be incrementally modified to use bounds checking.

Yarra \cite{Schlesinger2011} proposes an extension to C to prevent data
modification and data disclosure attacks involving important data. 
Programmers can declare types as ``critical'' types.  All other types
are ``non-critical'' types.  A pointer to a critical type \var{T} can 
be used to access only objects with runtime type \var{T}.   Conversely, pointers
to non-critical types cannot be used to access critical objects. 
The locations and types of objects of critical types are 
tracked at runtime.  Runtime checks protect critical objects from modification
by out-of-bounds memory writes.  If the source code for a program and the libraries
it uses are available, they can be be recompiled with runtime checks at every memory
access.  Yarra places objects of critical type in separate areas of memory from
non-critical types. For calls to unmodified libraries, Yarra uses virtual memory
protection to prevent the libraries from accessing critical data.

Failure-oblivious computing \cite{Rinard2004} proposes a different approach for
handling out-of-bounds memory accesses than what is propposed for Checked C.  
With Checked C,
the program will be terminated or an error handler will be invoked.  If the
program terminates, this converts data modification and data disclosure attacks to denial-of-service attacks.  Rinard {\it et al.} suggest discarding out-of-bounds
writes and converting out-of-bounds reads to small integer values, cycling
through a sequence of small integer values.   They implement the runtime
bounds checking suggested by \cite{Ruwase2004} and show that their approach
improves the availablity of servers with memory corruption errors.

